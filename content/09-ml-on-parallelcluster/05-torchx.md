---
title : "f. TorchX: multi platform deployment tool for distributed training"
date: 2020-09-04T15:58:58Z
weight : 30
tags : ["training", "data parallel", "ML", "sbatch", "slurm", "multi node", "multi gpu"]
---
 
TorchX support [slurm scheduler](https://pytorch.org/torchx/latest/schedulers/slurm.html). We can use it to easily deploy PyTorch distributed workloads accross different schedulers.