---
title: "Megatron-LM"
weight: 50
pre: "<b>Part IV ‚ÅÉ </b>"
tags: ["HPC", "Overview"]
---

Next we'll run [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) ([paper](https://arxiv.org/pdf/1909.08053.pdf)), a powerful framework from NVIDIA for training multi-billion parameter models.

We'll be using the `pretrain_gpt_distributed.py` functionality to train a **7.5B** parameter model similar to GPT.

![Achieved Petaflops](/images/04-Megatron-LM/Achieved_petaFLOPs.png)